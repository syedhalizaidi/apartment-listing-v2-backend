# app/services/agent.py
import os
import re
import json
from dataclasses import dataclass, field
from typing import Tuple, List, Dict, Any, Optional, Union
import tempfile  # [NEW]
import shutil    # [NEW]
import mimetypes # [NEW]
import requests
from .vector_store import ListingStore
from .llm import parse_prefs  # optional helper; we merge its output
from .llm import get_gpt_response
from io import BytesIO  # [NEW]


import base64  # [NEW]
from PIL import Image, ImageDraw, ImageFont  # <-- add Draw, Font
  # you already import Image; include Draw, Font

from .session_store import SessionStore

session_store = SessionStore()

SEARCH_COLLECTION = os.getenv("VECTOR_COLLECTION", "listings_openai")


# ---------- Debug helper ----------
def dbg(label: str, data: Any = None):
    try:
        if data is None:
            print(f"DEBUG: {label}")
        else:
            snippet = data
            try:
                snippet = json.dumps(data, ensure_ascii=False)[:1500]
            except Exception:
                snippet = str(data)[:1500]
            print(f"DEBUG: {label}: {snippet}")
    except Exception:
        pass
# ======== Runtime-configurable prompts & directives ========
# add near imports
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def _session_with_retries() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=4,                # total attempts
        connect=4,              # include DNS/connect
        read=4,
        backoff_factor=0.6,     # 0.6, 1.2, 2.4, 4.8s
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=["GET", "HEAD", "OPTIONS"]
    )
    s.mount("http://", HTTPAdapter(max_retries=retry))
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.headers.update({
        "User-Agent": os.getenv("IMG_FETCH_UA", "Mozilla/5.0 (compatible; ListingBot/1.0)"),
        "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
    })
    timeout = float(os.getenv("IMG_FETCH_TIMEOUT", "15"))
    s.request_timeout = timeout
    return s

@dataclass
class BotPrompts:
    # Minimal fallbacks; most now AI-handled
    contact_saved: str = "Perfect ‚Äî I‚Äôve saved your contact details."
    final_confirmation: str = (
        "All set! I‚Äôve noted your interest in:\n{summary}\n\n"
        "Requested time: {time}\n"
        "Contact: {name} | {email} | {phone}\n\n"
        "I‚Äôll reach out and update you. If anything changes, just tell me here."
    )
    scope_decline: str = (
        "I‚Äôm focused on apartment hunting, so I can‚Äôt help with that topic. "
        "If you‚Äôd like, I can keep searching rentals or refine your preferences."
    )

    @staticmethod
    def from_dict(d: Optional[Dict[str, str]]) -> "BotPrompts":
        bp = BotPrompts()
        if not d:
            return bp
        for k, v in d.items():
            if hasattr(bp, k) and isinstance(v, str) and v.strip():
                setattr(bp, k, v)
        return bp

from io import BytesIO

def _add_caption_to_bytes(raw: bytes, text: str = "disruptive thoughts generation") -> bytes:
    im = Image.open(BytesIO(raw)).convert("RGB")
    draw = ImageDraw.Draw(im)
    try:
        font_path = os.getenv("CAPTION_FONT_PATH")
        if font_path and os.path.exists(font_path):
            size = max(12, int(min(im.size) * float(os.getenv("CAPTION_FONT_SCALE", "0.02"))))
            font = ImageFont.truetype(font_path, size=size)
        else:
            font = ImageFont.load_default()
    except Exception:
        font = ImageFont.load_default()

    try:
        bbox = draw.textbbox((0, 0), text, font=font)
        tw, th = bbox[2]-bbox[0], bbox[3]-bbox[1]
    except Exception:
        tw, th = draw.textsize(text, font=font)

    margin = int(os.getenv("CAPTION_MARGIN", "10"))
    x = im.width - tw - margin
    y = im.height - th - margin

    # shadow + white
    draw.text((x+1, y+1), text, fill=(0,0,0), font=font)
    color = tuple(int(c) for c in os.getenv("CAPTION_COLOR", "255,255,255").split(","))
    draw.text((x, y), text, fill=color, font=font)

    out = BytesIO()
    im.save(out, format="PNG", optimize=True)
    return out.getvalue()

def _add_caption_to_file(path: str, text: str = "disruptive thoughts generation") -> str:
    try:
        with open(path, "rb") as f:
            raw = f.read()
        new_bytes = _add_caption_to_bytes(raw, text)
        out_path = os.path.join(os.path.dirname(path), f"cap_{os.path.splitext(os.path.basename(path))[0]}.png")
        with open(out_path, "wb") as o:
            o.write(new_bytes)
        return out_path
    except Exception as e:
        dbg("caption_add_failed", {"file": path, "err": str(e)})
        return path


def _lightly_process_images(filepaths: List[str]) -> List[str]:
    """
    Safely standardize images IF PIL is available:
      - Strip EXIF/metadata
      - Convert to RGB
      - Re-encode to JPEG/PNG with sane quality
    NOTE: This is NOT for evading copyright; only use on licensed images.
    Controlled by env: PROCESS_IMAGES=1 to enable.
    """
    if os.getenv("PROCESS_IMAGES", "1") not in ("1", "true", "yes"):
        return filepaths

    if Image is None:
        dbg("pillow_unavailable", {})
        return filepaths

    processed: List[str] = []
    for p in filepaths:
        try:
            with Image.open(p) as im:
                fmt = (im.format or "").upper()
                im = im.convert("RGB")

                # Choose format: keep PNG if it was PNG and has transparency, else JPEG
                out_ext = ".jpg"
                save_kwargs = {"quality": int(os.getenv("JPEG_QUALITY", "88")), "optimize": True}
                if fmt == "PNG":
                    try:
                        # if alpha present, keep PNG
                        if "A" in im.getbands():
                            out_ext = ".png"
                            save_kwargs = {"optimize": True}
                    except Exception:
                        pass

                out_path = os.path.join(os.path.dirname(p), f"proc_{os.path.basename(p).split('.')[0]}{out_ext}")
                # Save to strip metadata
                im.save(out_path, **save_kwargs)
                processed.append(out_path)
        except Exception as e:
            dbg("image_process_failed", {"file": p, "err": str(e)})
            processed.append(p)  # fall back to original file for this one
    return processed
import base64  # at top if not present
import threading, time, uuid, errno

def _ensure_dir(path: str) -> Optional[str]:
    """Try to mkdir, return usable path or None."""
    try:
        os.makedirs(path, exist_ok=True)
        # sanity: actually writable?
        testfile = os.path.join(path, f".wtest_{uuid.uuid4().hex}")
        with open(testfile, "wb") as tf:
            tf.write(b"ok")
        os.remove(testfile)
        return path
    except Exception as e:
        dbg("mkdir_not_writable", {"path": path, "err": str(e)})
        return None

def _pick_local_publish_dir() -> Optional[str]:
    # 1) user-provided
    cfg = (os.getenv("MEDIA_LOCAL_DIR") or "").strip()
    if cfg:
        ok = _ensure_dir(cfg)
        if ok:
            return ok
    # 2) ./static/wa-media (project relative)
    proj_static = os.path.join(os.getcwd(), "static", "wa-media")
    ok = _ensure_dir(proj_static)
    if ok:
        return ok
    # 3) /tmp/wa-media
    tmpdir = os.path.join(tempfile.gettempdir(), "wa-media")
    ok = _ensure_dir(tmpdir)
    if ok:
        return ok
    return None
def _ensure_dir_strict(path: str) -> bool:
    try:
        os.makedirs(path, exist_ok=True)
        testfile = os.path.join(path, f".wtest_{uuid.uuid4().hex}")
        with open(testfile, "wb") as tf:
            tf.write(b"ok")
        os.remove(testfile)
        return True
    except Exception as e:
        dbg("media_dir_not_writable", {"path": path, "err": str(e)})
        return False

def _publish_locally(filepaths: List[str]) -> List[str]:
    """
    Copy captioned files into a locally served public dir and return ABSOLUTE URLs.
    Uses MEDIA_LOCAL_DIR (default ./static/wa-media) + MEDIA_BASE_URL or SERVER_BASE_URL.
    """
    local_dir = (os.getenv("MEDIA_LOCAL_DIR") or "./static/wa-media").strip()
    base_url  = (os.getenv("MEDIA_BASE_URL") or "").strip().rstrip("/")
    ttl       = int(os.getenv("MEDIA_TTL_SECONDS", "900"))

    if not (base_url.startswith("http://") or base_url.startswith("https://")):
        server = (os.getenv("SERVER_BASE_URL") or "").strip().rstrip("/")
        if server:
            base_url = server + "/static/wa-media"
        else:
            dbg("media_base_url_invalid", {"MEDIA_BASE_URL": base_url})
            return []

    if not _ensure_dir_strict(local_dir):
        return []

    out_urls, to_delete = [], []
    for src in filepaths:
        try:
            stem, ext = os.path.splitext(os.path.basename(src))
            ext = (ext or ".png").lower()
            unique = f"{stem}_{int(time.time())}_{uuid.uuid4().hex[:8]}{ext}"
            dst = os.path.join(local_dir, unique)
            _atomic_copy(src, dst)  # üîí atomic write
            out_urls.append(f"{base_url}/{unique}")
            to_delete.append(dst)
        except Exception as e:
            dbg("local_publish_failed", {"src": src, "err": str(e)})

    # delayed delete (Twilio may re-fetch)
    if to_delete and ttl > 0:
        def _job(paths, delay):
            time.sleep(delay)
            for p in paths:
                try: os.remove(p)
                except Exception: pass
        threading.Thread(target=_job, args=(to_delete, ttl), daemon=True).start()

    return out_urls



def _ai_edit_images_if_allowed(filepaths: List[str]) -> List[str]:
    """
    Always add 'disruptive thoughts generation' text into each image (bottom-right),
    regardless of OpenAI usage. Prevents sending uncaptained images.
    """
    caption_text = os.getenv("CAPTION_TEXT", "disruptive thoughts generation")

    use_openai = os.getenv("USE_OPENAI_IMAGE_EDIT", "0").lower() in ("1", "true", "yes")

    out_files: List[str] = []
    for p in filepaths:
        try:
            # 1Ô∏è‚É£ Try AI edit (optional, not required)
            raw_bytes = None
            if use_openai:
                from openai import OpenAI
                client = OpenAI()
                with open(p, "rb") as f:
                    resp = client.images.edit(
                        model="gpt-image-1",
                        image=f,
                        prompt="Lightly normalize image; preserve content and composition."
                    )
                if getattr(resp, "data", None) and getattr(resp.data[0], "b64_json", None):
                    raw_bytes = base64.b64decode(resp.data[0].b64_json)

            # 2Ô∏è‚É£ Always add caption (even if OpenAI edit failed)
            if raw_bytes:
                new_bytes = _add_caption_to_bytes(raw_bytes, caption_text)
            else:
                new_bytes = _add_caption_to_file(p, caption_text)
                out_files.append(new_bytes)
                continue

            # 3Ô∏è‚É£ Save edited+captioned image
            out_path = os.path.join(os.path.dirname(p), f"cap_{os.path.splitext(os.path.basename(p))[0]}.png")
            with open(out_path, "wb") as o:
                o.write(new_bytes)
            out_files.append(out_path)

        except Exception as e:
            dbg("ai_edit_failed_force_caption", {"file": p, "err": str(e)})
            try:
                out_files.append(_add_caption_to_file(p, caption_text))
            except Exception as e2:
                dbg("caption_fallback_failed", {"file": p, "err": str(e2)})
                out_files.append(p)

    return out_files



@dataclass
class BotSettings:
    system_commands: List[str] = field(default_factory=lambda: [
        "You are Kaebo‚Äôs AI assistant for apartment hunting. All interactions must be conversational, not scripted. "
        "The user should feel like they are chatting with their very close friend who is also a very successful real estate agent. "
        "Perfect mix of casual, friendly, and professional. Respond in the user's language. "
        "Stay on-topic: apartment hunting in Japan only. If off-topic, politely steer back. "
        "Be concise, natural, and engaging. For greetings, warmly introduce yourself and ask key questions conversationally. "
        "If no apartments in requested area (e.g., outside Japan), apologize and suggest Japan options."
    ])
    prompts: BotPrompts = field(default_factory=BotPrompts)
    model: str = "gpt-4o-mini"
    history_window: int = 10
    temperature: float = float(os.getenv("LLM_TEMPERATURE", "0.1"))
    top_p: float = float(os.getenv("LLM_TOP_P", "1.0"))
    seed: Optional[int] = int(os.getenv("LLM_SEED", "0")) if os.getenv("LLM_SEED") else None

    @staticmethod
    def from_inputs(system_commands: Optional[List[str]], prompts: Optional[Dict[str, str]], model: Optional[str] = None,
                    history_window: Optional[int] = None) -> "BotSettings":
        return BotSettings(
            system_commands=(system_commands or BotSettings().system_commands),
            prompts=BotPrompts.from_dict(prompts),
            model=model or "gpt-4o-mini",
            history_window=history_window or 10
        )

# ======== OpenAI helpers =========

def _to_messages(system: Union[str, List[str]], extra_system: Optional[Union[str, List[str]]]) -> List[Dict[str, str]]:
    messages = []
    if isinstance(system, list):
        for s in system:
            messages.append({"role": "system", "content": s})
    else:
        messages.append({"role": "system", "content": system})
    if extra_system:
        if isinstance(extra_system, list):
            for s in extra_system:
                messages.append({"role": "system", "content": s})
        else:
            messages.append({"role": "system", "content": extra_system})
    return messages


def chat_json(
    user_id: str,
    system: Union[str, List[str]],
    payload: Dict[str, Any],
    model: str = "gpt-4o-mini",
    extra_system: Optional[Union[str, List[str]]] = None,
    history_window: int = 10,
    temperature: float = 0.1,
    top_p: float = 1.0,
    seed: Optional[int] = None
) -> Dict[str, Any]:
    try:
        from openai import OpenAI
        client = OpenAI()
    except Exception as e:
        dbg("OpenAI client unavailable", str(e))
        return {"error": f"OpenAI unavailable: {e}", "echo": payload}

    sess = ensure_session(user_id)
    history: List[Dict[str, str]] = sess.get("history", [])

    messages = _to_messages(system, extra_system)

    for h in history[-history_window:]:
        role = h.get("role", "user")
        content = h.get("content", "")
        if content:
            messages.append({"role": role, "content": content})

    try:
        messages.append({"role": "user", "content": json.dumps(payload, ensure_ascii=False)})
    except Exception:
        messages.append({"role": "user", "content": str(payload)})

    dbg("OpenAI request messages", messages)

    kwargs = {
        "model": model,
        "messages": messages,
        "response_format": {"type": "json_object"},
        "temperature": float(temperature),
        "top_p": float(top_p)
    }
    if seed is not None:
        try:
            kwargs["seed"] = int(seed)
        except Exception:
            pass
    resp = client.chat.completions.create(**kwargs)
    content = resp.choices[0].message.content or "{}"
    dbg("OpenAI response content", content)
    try:
        return json.loads(content)
    except Exception:
        return {"error": "json_parse_error", "raw": content}


# ===== JP translation & intent routing =====

def translate_query_to_japanese(user_id: str, query: str, areas: List[str], settings: BotSettings) -> Dict[str, Any]:
    system = [
        "You convert rental apartment searches into concise Japanese search keywords "
        "appropriate for Japanese real-estate listings. Return ONLY JSON."
    ] + (settings.system_commands or [])
    payload = {
        "query_en": query,
        "areas_en": areas,
        "instructions": (
            "Translate succinctly. Prefer tokens like: Ë≥ÉË≤∏, „Ç¢„Éë„Éº„Éà, LDK/1K, ÂÆ∂Ë≥É. "
            "Output JSON: {\"query_jp\":\"...\", \"areas_jp\":[\"...\"]}. "
            "If there are no areas, return an empty array for areas_jp."
        )
    }
    res = chat_json(
        user_id,
        system,
        payload,
        model=settings.model,
        extra_system='Return only JSON with keys "query_jp" and "areas_jp".',
        history_window=settings.history_window,
        temperature=settings.temperature,
        top_p=settings.top_p,
        seed=settings.seed
    )
    if not isinstance(res, dict):
        return {"query_jp": "", "areas_jp": []}
    return {
        "query_jp": res.get("query_jp") or "",
        "areas_jp": res.get("areas_jp") or []
    }


def route_intent_with_llm(user_id: str, message: str, settings: BotSettings) -> Dict[str, Any]:
    system = [
        "You are a conversational router for a rental apartment assistant. "
        "Decide what the user is doing and extract structured data. Return ONLY JSON."
    ] + (settings.system_commands or [])
    schema_note = (
        "Return a JSON object with keys: "
        "{"
        "\"intent\":\"greeting|search|select|time|contact|clarify|other\","
        "\"query\":\"optional string for search\","
        "\"selection\":[numbers],"
        "\"time\":\"optional time expression like '14:00' or 'tomorrow afternoon'\","
        "\"prefs\":{"
        "  \"areas\":[\"...\"],"
        "  \"max_budget_usd\":number|null,"
        "  \"max_budget_yen\":number|null,"
        "  \"bedrooms\":number|array|null,"
        "  \"dog_friendly\":true|false|null"
        "},"
        "\"needs_clarification\":true|false"
        "}"
    )
    sess = ensure_session(user_id)
    session_snapshot = {
        "prefs": sess.get("prefs", {}),
        "awaiting_time": bool(sess.get("awaiting_time")),
        "awaiting_contact": bool(sess.get("awaiting_contact")),
        "chosen_count": len(sess.get("chosen") or []),
        "last_results_count": len(sess.get("last_results") or []),
    }
    payload = {
        "user_message": message,
        "session": session_snapshot,
        "instruction": (
            "Classify the user's latest message. "
            "If it contains numbered lines (1..5), interpret them. "
            "If selecting by number, put those numbers in 'selection'. "
            "If giving a time, extract to 'time'. "
            "If asking to see listings, intent='search' and set a concise 'query' "
            "using any areas/stations/budget hints you see (include 'apartment' and 'rent'). "
            "Always fill 'prefs' with whatever you can glean; leave missing as null or empty."
        ),
        "output_schema": schema_note
    }
    result = chat_json(
        user_id,
        system,
        payload,
        model=settings.model,
        extra_system=None,
        history_window=settings.history_window,
        temperature=settings.temperature,
        top_p=settings.top_p,
        seed=settings.seed
    )
    dbg("Router result", result)

    if not isinstance(result, dict):
        result = {}
    result.setdefault("intent", "other")
    result.setdefault("prefs", {})
    result.setdefault("selection", [])
    return result


def compose_reply_with_llm(user_id: str, brief: Dict[str, Any], settings: BotSettings) -> str:
    system = settings.system_commands  # Use full system for tone
    # enrich brief with session snapshot for model-driven behavior
    sess = ensure_session(user_id)
    brief = dict(brief)
    brief["session"] = {
        "prefs": sess.get("prefs", {}),
        "awaiting_time": bool(sess.get("awaiting_time")),
        "awaiting_contact": bool(sess.get("awaiting_contact")),
        "chosen_count": len(sess.get("chosen") or []),
        "last_results_count": len(sess.get("last_results") or []),
        "last_user": (sess.get("history") or [{}])[-1:] or []
    }
    result = chat_json(
        user_id,
        system,
        {"brief": brief},
        model=settings.model,
        extra_system='Return only JSON with {"text":"..."}. Keep it conversational and natural.',
        history_window=settings.history_window,
        temperature=settings.temperature,
        top_p=settings.top_p,
        seed=settings.seed
    )
    text = ""
    if isinstance(result, dict):
        text = result.get("text") or ""
    if not text:
        return brief.get("fallback", "Got it ‚Äî let's keep hunting for your perfect spot!")
    return text


# ===== Util / session helpers =====
def ensure_session(user_id: str) -> Dict[str, Any]:
    sess = session_store.get(user_id)
    if sess is None:
        sess = {
            "asked_clarify": False,
            "prefs": {
                "bedrooms": None,
                "max_budget_usd": None,
                "max_budget_yen": None,
                "areas": [],
                "dog_friendly": None
            },
            "last_results": [],
            "chosen": [],
            "awaiting_time": False,
            "awaiting_contact": False,
            "appt_time": None,
            "contact": {"name": None, "email": None, "phone": None},
            "has_initialized": False,
            "history": []
            ,
            "search_cache": {}
        }
        session_store.set(user_id, sess)
    return sess


def is_greeting(message: str) -> bool:
    greetings = ["hi", "hello", "hey", "greetings", "good morning", "good afternoon", "good evening", "hi there"]
    low = (message or "").lower()
    return any(g in low for g in greetings)


# ===== Simple location extraction (EN/JP) =====
CITY_ALIASES = {
    "tokyo": ["tokyo", "Êù±‰∫¨", "„Å®„ÅÜ„Åç„Çá„ÅÜ", "23Âå∫"],
    "osaka": ["osaka", "Â§ßÈò™", "Â§ßÈò™Â∏Ç", "„Åä„Åä„Åï„Åã"],
    "kyoto": ["kyoto", "‰∫¨ÈÉΩ", "„Åç„Çá„ÅÜ„Å®"],
    "sapporo": ["sapporo", "Êú≠Âπå", "„Åï„Å£„ÅΩ„Çç"],
    "fukuoka": ["fukuoka", "Á¶èÂ≤°", "„Åµ„Åè„Åä„Åã"],
    "nagoya": ["nagoya", "ÂêçÂè§Â±ã", "„Å™„Åî„ÇÑ"],
    "yokohama": ["yokohama", "Ê®™Êµú", "„Çà„Åì„ÅØ„Åæ"],
}
# Normalize any phone into Twilio WhatsApp format
def _normalize_wa_number(raw: Optional[str]) -> Optional[str]:
    if not raw:
        return None
    s = str(raw).strip()
    # If already 'whatsapp:+123...' just return
    if s.startswith("whatsapp:"):
        return s
    # If E.164 +123..., prefix whatsapp:
    if s.startswith("+"):
        return f"whatsapp:{s}"
    # If it's just digits or mixed, extract digits
    digits = re.sub(r"\D", "", s)
    if digits:
        # assume E.164 without '+'
        if not digits.startswith("+"):
            return f"whatsapp:+{digits}"
        return f"whatsapp:{digits}"
    return None

def extract_locations_from_text(text: str) -> List[str]:
    if not text:
        return []
    t = text.lower()
    found = []
    m = re.findall(r"\bin\s+([a-z\u3040-\u30ff\u4e00-\u9faf][\w\u3040-\u30ff\u4e00-\u9faf\s-]{1,30})", t)
    explicit = [x.strip(" .,!?:;") for x in m] if m else []
    for city, aliases in CITY_ALIASES.items():
        if any(a in t for a in aliases):
            found.append(city.capitalize())
    prioritized = []
    for frag in explicit:
        for city, aliases in CITY_ALIASES.items():
            if any(a in frag for a in aliases):
                prioritized.append(city.capitalize())
    result = []
    for lst in (prioritized, found):
        for c in lst:
            if c not in result:
                result.append(c)
    return result


# ===== Formatting =====
def format_listings_for_user(listings: List[Dict[str, Any]]) -> str:
    lines = []
    for i, it in enumerate(listings, start=1):
        price = None
        if it.get("price_usd"):
            price = f"${it.get('price_usd')}/mo"
        elif it.get("price_yen"):
            try:
                price = f"¬•{int(it.get('price_yen')):,}/mo"
            except Exception:
                price = f"¬•{it.get('price_yen')}/mo"

        bd = f"{it.get('bedrooms')}bd" if it.get("bedrooms") is not None else ""
        loc = ", ".join([x for x in [it.get("neighborhood"), it.get("city")] if x])
        url = it.get("url") or it.get("details_url") or it.get("source") or ""
        title = it.get("title") or "Untitled"

        line = f"{i}) {title}"
        if price:
            line += f" ‚Äî {price}"
        if bd:
            line += f" ‚Äî {bd}"
        if loc:
            line += f" ‚Äî {loc}"
        # show a single-line description if we have it
        desc = it.get("description")
        if not desc:
            raw = it.get("document") or it.get("text") or ""
            raw = re.sub(r"https?://\S+", "", raw)
            raw = re.sub(r"\s+", " ", raw).strip()
            if raw:
                desc = (raw[:220] + "‚Ä¶") if len(raw) > 220 else raw
        if desc:
            desc = re.sub(r"\s+", " ", desc).strip()
            line += f"\n{desc}"

        if url:
            line += f"\n{url}"
        if it.get("pet_dog"):
            line += "  (dog OK)"
        lines.append(line)
    return "\n\n".join(lines)


# ===== Query expansion =====
def _money_jpy(n: Optional[int]) -> str:
    try:
        return f"{int(n):,}"
    except Exception:
        return str(n) if n is not None else ""

def expand_query(base: str, prefs: Dict[str, Any]) -> List[str]:
    areas = [a for a in (prefs.get("areas") or []) if isinstance(a, str) and a.strip()]
    yen = prefs.get("max_budget_yen")
    bds = prefs.get("bedrooms")
    bed_word = None
    if isinstance(bds, int):
        bed_word = "1K" if bds == 1 else f"{bds}LDK"
    elif isinstance(bds, list) and bds:
        mx = max(bds)
        bed_word = "1K" if mx == 1 else f"{mx}LDK"

    budget_s = _money_jpy(yen)
    en = (base or "").strip()

    variants: List[str] = []
    if en:
        variants.append(en)

    # EN-focused with areas
    if areas:
        v1 = " ".join([en or "apartment rent", *areas, f"under {yen} yen" if yen else ""])
        variants.append(v1.strip())
    else:
        variants.append((en or "apartment rent").strip())

    # JP-ish tokens
    jp_bits = ["Ë≥ÉË≤∏", "„Ç¢„Éë„Éº„Éà"]
    if bed_word: jp_bits.append(bed_word)
    if yen: jp_bits.append(f"ÂÆ∂Ë≥É {budget_s}ÂÜÜ ‰ª•‰∏ã")
    if areas: jp_bits.extend(areas)
    variants.append(" ".join(jp_bits).strip())

    # Mixed bilingual
    mix = []
    if areas: mix += areas
    mix += ["apartment rent", "Ë≥ÉË≤∏"]
    if yen: mix.append(f"<=¬•{budget_s}")
    if bed_word: mix.append(bed_word)
    variants.append(" ".join(mix).strip())

    # Dedup
    out, seen = [], set()
    for v in variants:
        v2 = re.sub(r"\s+", " ", v or "").strip()
        if v2 and v2.lower() not in seen:
            seen.add(v2.lower()); out.append(v2)
    dbg("Expanded queries", out)
    return out[:6]


# ===== Candidate coercion (fallback) =====
URL_RX = re.compile(r"https?://\S+")
YEN_RX = re.compile(r"[¬•¬•\u00A5]?\s?(\d{2,7})\s?(?:ÂÜÜ|yen)?", re.IGNORECASE)

def coerce_candidates_to_listings(cands: List[Dict[str, Any]], want: int = 5) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for c in cands:
        it = {}
        for k in ("title","url","details_url","source","price_usd","price_yen","bedrooms","neighborhood","city","pet_dog"):
            if k in c and c.get(k) not in (None,"",[]):
                it[k] = c[k]

        doc = c.get("document") or c.get("text") or ""
        if doc and not it.get("title"):
            m = re.search(r"^\s*(?:\[.*?\]\s*)?(.+?)(?:¬∑|-|‚Äî|\n|$)", doc)
            if m: it["title"] = m.group(1).strip()[:120]
        if doc and not it.get("price_yen"):
            m = YEN_RX.search(doc)
            if m:
                try: it["price_yen"] = int(m.group(1))
                except Exception: pass
        if not it.get("url"):
            m = URL_RX.search(doc)
            if m: it["url"] = m.group(0)
        # Add a compact description if available
        if doc and not it.get("description"):
            no_urls = re.sub(r"https?://\S+", "", doc)
            one_line = re.sub(r"\s+", " ", no_urls).strip()
            if one_line:
                it["description"] = (one_line[:220] + "‚Ä¶") if len(one_line) > 220 else one_line


        if it:
            out.append(it)
        if len(out) >= want:
            break
    return out

def _backfill_media_from_hits(cleaned: List[Dict[str, Any]], hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    If the cleaner dropped image fields, copy them from the original hits by matching url/id.
    """
    def media_keys(d: Dict[str, Any]) -> List[str]:
        return [k for k in ["image","image_url","image_urls","images","photos","thumbnails"] if d.get(k)]

    # Build quick lookup from original hits
    by_url = {}
    by_id = {}
    for h in hits:
        u = h.get("url") or h.get("details_url") or h.get("source_url")
        if u:
            by_url[u] = h
        if h.get("id"):
            by_id[h["id"]] = h

    # Backfill
    fixed = []
    for it in cleaned:
        needs = len(media_keys(it)) == 0
        if needs:
            src = None
            if it.get("id") and it["id"] in by_id:
                src = by_id[it["id"]]
            if not src:
                u = it.get("url") or it.get("details_url") or it.get("source_url")
                if u and u in by_url:
                    src = by_url[u]
            if src:
                for k in ["image","image_url","image_urls","images","photos","thumbnails"]:
                    if src.get(k) and not it.get(k):
                        it[k] = src[k]
        fixed.append(it)
    return fixed

# ===== Search helpers =====
def _search_once(store: ListingStore, variants: List[str]) -> List[Dict[str, Any]]:
    all_candidates: List[Dict[str, Any]] = []
    seen_ids = set()
    for q in variants:
        dbg("Search query", q)
        # pass-through where filters later at caller; here focus on recall
        cands = store.search(q, n=80) or []
        dbg("Candidates found", len(cands))
        for c in cands:
            key = c.get("id") or c.get("url") or c.get("details_url") or c.get("source") or c.get("document") or str(c)[:200]
            if key not in seen_ids:
                seen_ids.add(key); all_candidates.append(c)
        if len(all_candidates) >= 180:
            break
    dbg("Merged candidates total", len(all_candidates))
    # deterministic ordering by score desc then id/url asc
    def kf(x):
        sid = x.get("id") or x.get("url") or x.get("details_url") or ""
        return (-float(x.get("_score", 0.0)), str(sid))
    try:
        all_candidates.sort(key=kf)
    except Exception:
        pass
    return all_candidates


def _area_tokens(areas: List[str]) -> List[str]:
    toks: List[str] = []
    for a in areas:
        low = a.lower().strip()
        toks.append(low)
        for city, aliases in CITY_ALIASES.items():
            if low == city or low in aliases:
                toks.extend([al for al in aliases])
    out = []
    seen = set()
    for t in toks:
        t2 = t.replace(" ", "")
        if t2 and t2 not in seen:
            seen.add(t2); out.append(t2)
    return out


def _matches_area(item: Dict[str, Any], area_tokens: List[str]) -> bool:
    if not area_tokens:
        return True
    
    # Get location fields and normalize
    location_fields = [
        item.get('neighborhood', '').lower().replace(" ", "").replace("-", ""),
        item.get('city', '').lower().replace(" ", "").replace("-", ""),
        item.get('prefecture', '').lower().replace(" ", "").replace("-", "")
    ]
    
    # Check each token against each location field
    for tok in area_tokens:
        # Remove common suffixes like -shi, -ku, -machi for more flexible matching
        clean_tok = re.sub(r'-(shi|ku|machi|cho|mura)$', '', tok.lower().replace(" ", ""))
        for loc in location_fields:
            if clean_tok in loc or loc in clean_tok:
                return True
    
    # Also check in document text if available
    doc = (item.get("document") or item.get("text") or "").lower().replace(" ", "").replace("-", "")
    if doc:
        for tok in area_tokens:
            clean_tok = re.sub(r'-(shi|ku|machi|cho|mura)$', '', tok.lower().replace(" ", ""))
            if clean_tok in doc:
                return True
                
    return False


# ===== Search pipeline =====
NON_JAPAN_LOCATIONS = ["china", "usa", "us", "europe", "london", "new york", "paris"]  # Expand as needed

def search_and_clean(user_id: str, query: str, prefs: Dict[str, Any], settings: BotSettings, min_count: int = 3) -> Dict[str, Any]:
    store = ListingStore(collection=SEARCH_COLLECTION)
    dbg("Vector index/ns", {"index": getattr(store, "index_name", "unknown"), "ns": getattr(store, "namespace", "")})

    # cache key considers normalized variants and normalized prefs
    def _norm_prefs(p: Dict[str, Any]) -> Dict[str, Any]:
        areas = [a.strip().lower() for a in (p.get("areas") or []) if isinstance(a, str) and a.strip()]
        areas.sort()
        bds = p.get("bedrooms")
        if isinstance(bds, list) and bds:
            try:
                bds = max([int(x) for x in bds if x is not None])
            except Exception:
                bds = None
        elif isinstance(bds, int):
            bds = int(bds)
        else:
            bds = None
        out = {
            "areas": areas,
            "max_budget_usd": p.get("max_budget_usd"),
            "max_budget_yen": p.get("max_budget_yen"),
            "bedrooms": bds,
            "dog_friendly": p.get("dog_friendly")
        }
        return out

    sess = ensure_session(user_id)
    cache = sess.get("search_cache", {})
    variants = expand_query(query, prefs)
    key_variants = sorted([v.lower() for v in variants])
    cache_key = json.dumps({"vars": key_variants, "prefs": _norm_prefs(prefs)}, sort_keys=True, ensure_ascii=False)
    if cache_key in cache:
        dbg("search_cache_hit", {"vars": key_variants})
        return cache[cache_key]

    # Check for non-Japan locations
    areas_in = [a for a in (prefs.get("areas") or []) if isinstance(a, str) and a.strip()]
    query_lower = query.lower()
    if any(loc in query_lower or any(loc in a.lower() for a in areas_in) for loc in NON_JAPAN_LOCATIONS):
        dbg("Non-Japan location detected", {"query": query, "areas": areas_in})
        return {"candidates": [], "filtered": [], "cleaned": [], "final": [], "unavailable": True, "reason": "non_japan"}

    # use where filters to push down numeric filtering
    all_candidates = []
    for qv in variants:
        cs = store.search(qv, n=100, where=prefs) or []  # Increased n for more results on corrections
        all_candidates.extend(cs)
    # dedup
    seen = set(); merged=[]
    for c in all_candidates:
        key = c.get("id") or c.get("url") or c.get("details_url") or c.get("source") or c.get("document") or str(c)[:200]
        if key in seen: continue
        seen.add(key); merged.append(c)
    all_candidates = merged

    area_tokens = _area_tokens([a.lower() for a in areas_in])

    if areas_in:
        area_matches = [it for it in all_candidates if _matches_area(it, area_tokens)]
        dbg("Strict-area pass1 matches", len(area_matches))
        base_pool = area_matches
    else:
        base_pool = all_candidates

    if areas_in and len(base_pool) == 0:
        dbg("No matches for requested area; attempting JP retry", {"areas": areas_in})
        trans = translate_query_to_japanese(user_id, query, areas_in, settings)
        dbg("JP translation", trans)

        areas_jp = trans.get("areas_jp") or []
        if not areas_jp:
            for a in areas_in:
                low = a.lower()
                for city, aliases in CITY_ALIASES.items():
                    if low == city or low in aliases:
                        for al in aliases:
                            if re.search(r"[„ÅÅ-„Çì„Ç°-„É∂‰∏Ä-ÈæØ]", al):
                                areas_jp.append(al)

        prefs_jp = dict(prefs)
        prefs_jp["areas"] = areas_jp if areas_jp else areas_in
        q_jp = trans.get("query_jp") or query or "Ë≥ÉË≤∏ „Ç¢„Éë„Éº„Éà"

        variants_jp = expand_query(q_jp, prefs_jp)
        dbg("JP variants", variants_jp)

        all_candidates_jp = _search_once(store, variants_jp)
        area_tokens_jp = _area_tokens([a.lower() for a in (prefs_jp.get("areas") or [])])
        base_pool = [it for it in all_candidates_jp if _matches_area(it, area_tokens_jp)]
        dbg("Strict-area pass2 (JP) matches", len(base_pool))

        if len(base_pool) == 0:
            dbg("Unavailable after JP retry", {"areas": areas_in})
            return {"candidates": [], "filtered": [], "cleaned": [], "final": [], "unavailable": True}

    if not areas_in and len(base_pool) == 0 and len(all_candidates) == 0:
        trans = translate_query_to_japanese(user_id, query, [], settings)
        dbg("JP translation (no-area)", trans)
        q_jp = trans.get("query_jp") or "Ë≥ÉË≤∏ „Ç¢„Éë„Éº„Éà"
        variants_jp = expand_query(q_jp, prefs)
        dbg("JP variants (no-area)", variants_jp)
        base_pool = _search_once(store, variants_jp)

        if len(base_pool) == 0:
            dbg("Unavailable after no-area JP retry", {})
            return {"candidates": [], "filtered": [], "cleaned": [], "final": [], "unavailable": True}

    filtered = filter_results(base_pool, prefs)
    dbg("Filtered count", len(filtered))

    top_for_clean = filtered[:15] if filtered else base_pool[:15]  # Increased for more results
    cleaned = clean_results_with_llm(user_id, " / ".join(variants[:2]), prefs, top_for_clean, settings)[:15]
    # ‚úÖ Backfill any missing image fields from the originals
    cleaned = _backfill_media_from_hits(cleaned, top_for_clean)

    dbg("Cleaned results count", len(cleaned))

    results: List[Dict[str, Any]] = cleaned

    # Aim for a deterministic target count: prefer filtered size up to 15; otherwise base_pool size up to 15
    target_count = min(15, len(filtered) if filtered else len(base_pool))

    if len(results) < target_count:
        def keyf(x): return (x.get("id"), x.get("url"), x.get("details_url"), x.get("title"))
        have = {keyf(x) for x in results}
        for it in filtered:
            k = keyf(it)
            if k not in have:
                results.append(it); have.add(k)
            if len(results) >= target_count:
                break

    if len(results) < target_count:
        coerced = coerce_candidates_to_listings(base_pool, want=max(target_count, 15))
        def keyf2(x): return (x.get("id"), x.get("url"), x.get("details_url"), x.get("title"))
        have = {keyf2(x) for x in results}
        for it in coerced:
            if keyf2(it) not in have:
                results.append(it); have.add(keyf2(it))
            if len(results) >= target_count:
                break
        dbg("After coercion count", len(results))

    packed = {
        "candidates": base_pool,
        "filtered": filtered,
        "cleaned": cleaned,
        "final": results[:target_count] or results[:15],
        "unavailable": False
    }
    # store in cache
    try:
        cache[cache_key] = packed
        sess["search_cache"] = cache
        session_store.set(user_id, sess)
    except Exception:
        pass
    return packed


def filter_results(candidates: List[Dict[str, Any]], prefs: Dict[str, Any]) -> List[Dict[str, Any]]:
    out = []
    bds = set(prefs.get("bedrooms") or []) if isinstance(prefs.get("bedrooms"), list) else ({prefs["bedrooms"]} if isinstance(prefs.get("bedrooms"), int) else set())
    max_budget_usd = prefs.get("max_budget_usd")
    max_budget_yen = prefs.get("max_budget_yen")
    dog = prefs.get("dog_friendly")
    areas = [a.lower().replace(' ', '') for a in (prefs.get("areas") or [])]

    for it in candidates:
        item = it
        score = 0

        bd_val = item.get("bedrooms")
        if bds and isinstance(bd_val, int):
            score += 2 if bd_val in bds else 1

        if max_budget_usd is not None and item.get("price_usd") is not None:
            try:
                p = float(item["price_usd"])
                if p <= max_budget_usd: score += 2
                elif p <= max_budget_usd * 1.2: score += 1
            except Exception:
                pass

        if max_budget_yen is not None and item.get("price_yen") is not None:
            try:
                p = int(item["price_yen"])
                if p <= max_budget_yen: score += 2
                elif p <= int(max_budget_yen * 1.2): score += 1
            except Exception:
                pass

        if dog is True and item.get("pet_dog"):
            score += 1

        if areas:
            loc = f"{item.get('neighborhood','')}{item.get('city','')}".lower().replace(' ', '')
            doc = (item.get("document") or item.get("text") or "").lower().replace(" ", "")
            if any(a in loc for a in areas) or (doc and any(a in doc for a in areas)):
                score += 3
            else:
                score -= 1

        if score == 0:
            score = 0.1

        out.append({"item": item, "score": score})

    def sort_key(x):
        item = x["item"]
        usd = item.get("price_usd"); yen = item.get("price_yen")
        price = usd if usd is not None else (yen if yen is not None else float("inf"))
        tie = item.get("id") or item.get("url") or item.get("title") or ""
        return (-x["score"], price, str(tie))

    out.sort(key=sort_key)
    return [x["item"] for x in out]


def clean_results_with_llm(user_id: str, query: str, prefs: Dict[str, Any], hits: List[Dict[str, Any]], settings: BotSettings) -> List[Dict[str, Any]]:
    system = [
        "You are an assistant that normalizes rental listing data into a clean, consistent JSON schema.",
        "Return only JSON. No commentary."
    ] + (settings.system_commands or [])
    payload = {
        "query": query,
        "user_prefs": prefs,
        "listings": hits,
        # inside clean_results_with_llm(...)

        "expected_schema": {
            "id": "string",
            "title": "string",
            "url": "string",
            "price_usd": "number|optional",
            "price_yen": "number|optional",
            "bedrooms": "number|optional",
            "bathrooms": "number|optional",
            "neighborhood": "string|optional",
            "city": "string|optional",
            "pet_dog": "boolean|optional",
            "source": "string|optional",

            # ‚úÖ NEW: keep a short, plain-text description
            "description": "string|optional",

            # ‚úÖ Preserve media fields if present
            "image": "string|optional",
            "image_url": "string|optional",
            "image_urls": "array|string|optional",
            "images": "array|string|optional",
            "photos": "array|string|optional",
            "thumbnails": "array|string|optional"
        },
        "instructions": (
            "1) Normalize fields to the expected schema keys.\n"
            "2) If both USD and JPY exist, keep both; otherwise keep what‚Äôs available.\n"
            "3) Ensure url is a valid http(s) link if present.\n"
            "4) Remove nulls and unknowns rather than writing placeholders.\n"
            "5) Preserve any image fields if present under the provided keys; coerce single strings to arrays when sensible.\n"
            "6) If a human-readable summary exists in listing text, set 'description' to a clean, single-line summary (140‚Äì220 chars, no newlines, no URLs).\n"
            "7) Return a JSON object with {\"results\": [ ... up to 15 cleaned items ... ]}."
        )

    }
    result = chat_json(
        user_id,
        system,
        payload,
        model=settings.model,
        extra_system="If any field is missing, omit it.",
        history_window=settings.history_window
    )

    out = hits[:15]
    if isinstance(result, dict) and "results" in result and isinstance(result["results"], list):
        out = result["results"][:15]
    return out


# ===== Time utils (STRONG detection) =====
# We only treat input as time if:
#  - it has a colon hh:mm (optionally with am/pm), OR
#  - it has am/pm (e.g., 3am, 7 pm), OR
#  - it has explicit day-part words (today/tonight/morning/afternoon/evening/night), OR
#  - it's a range like '3-9' / '3 to 9' AND includes a day-part word. Bare '3' or '3-9' are NOT time.
_TIME_PART_WORDS = r"(today|tonight|tomorrow|morning|noon|afternoon|evening|night|tonight|tomorrow morning|tomorrow afternoon|tomorrow evening|tomorrow night)"
# hh:mm with optional am/pm
_TIME_HHMM_RE = re.compile(r"\b(?:[01]?\d|2[0-3]):[0-5]\d\s*(?:am|pm)?\b", re.IGNORECASE)
# h am/pm or hh am/pm
_TIME_AMPM_RE = re.compile(r"\b(?:[01]?\d|2[0-3])\s*(?:am|pm)\b", re.IGNORECASE)
# day-part words
_TIME_WORD_RE = re.compile(rf"\b{_TIME_PART_WORDS}\b", re.IGNORECASE)
# ranges that MUST include a day-part
_TIME_RANGE_STRONG_RE = re.compile(
    rf"\b(?P<start>[01]?\d|2[0-3])\s*(?:to|-|‚Äì|‚Äî)\s*(?P<end>[01]?\d|2[0-3])\b.*\b{_TIME_PART_WORDS}\b",
    re.IGNORECASE
)

def is_strong_time_expression(s: str) -> Optional[str]:
    if not s:
        return None
    txt = (s or "").strip()
    if _TIME_HHMM_RE.search(txt):
        return _TIME_HHMM_RE.search(txt).group(0)
    if _TIME_AMPM_RE.search(txt):
        return _TIME_AMPM_RE.search(txt).group(0)
    if _TIME_RANGE_STRONG_RE.search(txt):
        m = _TIME_RANGE_STRONG_RE.search(txt)
        start, end = m.group("start"), m.group("end")
        part = _TIME_WORD_RE.search(txt).group(0) if _TIME_WORD_RE.search(txt) else ""
        return f"{start}-{end} {part}".strip()
    if _TIME_WORD_RE.search(txt):
        return _TIME_WORD_RE.search(txt).group(0)
    return None


# ===== Pref merge helpers =====
def merge_prefs(old: Dict[str, Any], new_obj) -> Dict[str, Any]:
    try:
        new = new_obj.dict()
    except Exception:
        return old
    for k, v in new.items():
        if v is not None and v != []:
            old[k] = v
    return old

def _wait_until_public(
    urls: List[str],
    timeout: float = float(os.getenv("MEDIA_PROBE_TIMEOUT", "6.0")),
    interval: float = float(os.getenv("MEDIA_PROBE_INTERVAL", "0.25")),
) -> bool:
    """
    HEAD each URL until 200/206/304 (or small GET fallback) before we give Twilio the URL.
    """
    ok_codes = {200, 206, 304}
    t0 = time.time()
    pending = set(urls)
    headers = {"User-Agent": os.getenv("MEDIA_PROBE_UA", "MediaProbe/1.0")}
    while pending and (time.time() - t0) < timeout:
        done = set()
        for u in list(pending):
            try:
                r = requests.head(u, timeout=2.5, allow_redirects=True, headers=headers)
                if r.status_code in ok_codes:
                    done.add(u)
            except Exception:
                pass
        pending -= done
        if pending:
            time.sleep(interval)
    if pending:
        # Some stacks don‚Äôt implement HEAD well ‚Üí try GET
        for u in list(pending):
            try:
                r = requests.get(u, timeout=3.0, stream=True, headers=headers)
                if r.status_code in ok_codes:
                    pending.discard(u)
            except Exception:
                pass
    return len(pending) == 0

# ---------- FIX #1: Case-insensitive number words -> digits ----------
def normalize_digits(s: str) -> str:
    """
    Convert written numbers (one..twenty-nine) to digits, case-insensitively,
    and normalize dashes. Handles 'Twenty-One', 'twenty one', etc.
    """
    if not s:
        return ""
    txt = (s or "").replace("‚Äì", "-").replace("‚Äî", "-")

    # Replace specific 21..29 first to avoid partial overlaps
    specials = {
        r"\btwenty[-\s]?one\b": "21",
        r"\btwenty[-\s]?two\b": "22",
        r"\btwenty[-\s]?three\b": "23",
        r"\btwenty[-\s]?four\b": "24",
        r"\btwenty[-\s]?five\b": "25",
        r"\btwenty[-\s]?six\b": "26",
        r"\btwenty[-\s]?seven\b": "27",
        r"\btwenty[-\s]?eight\b": "28",
        r"\btwenty[-\s]?nine\b": "29",
    }
    for pat, rep in specials.items():
        txt = re.sub(pat, rep, txt, flags=re.IGNORECASE)

    mapping = {
        "zero": "0","one": "1","two": "2","three": "3","four": "4",
        "five": "5","six": "6","seven": "7","eight": "8","nine": "9",
        "ten":"10","eleven":"11","twelve":"12","thirteen":"13","fourteen":"14",
        "fifteen":"15","sixteen":"16","seventeen":"17","eighteen":"18","nineteen":"19","twenty":"20",
        "oh": "0","o": "0"
    }
    for word, digit in sorted(mapping.items(), key=lambda kv: -len(kv[0])):
        txt = re.sub(rf"\b{re.escape(word)}\b", digit, txt, flags=re.IGNORECASE)
    return txt
def _maybe_upload_from_urls(urls: List[str]) -> List[str]:
    """
    Optional uploader that accepts source URLs and returns rehosted public URLs.
    Requires URL_UPLOAD_ENDPOINT that accepts JSON {"source_url": "..."} -> {"url": "..."}.
    """
    endpoint = os.getenv("URL_UPLOAD_ENDPOINT")
    if not endpoint:
        return []
    out = []
    for u in urls:
        try:
            resp = requests.post(endpoint, json={"source_url": u}, timeout=30)
            resp.raise_for_status()
            data = resp.json() if resp.headers.get("content-type","").startswith("application/json") else {}
            new_url = data.get("url")
            if new_url and new_url.startswith(("http://","https://")):
                out.append(new_url)
            else:
                dbg("url_upload_no_url", {"source": u, "status": resp.status_code, "text": resp.text[:300]})
        except Exception as e:
            dbg("url_upload_failed", {"source": u, "err": str(e)})
    return out


# ===== Public bot entry points =====
def _atomic_copy(src: str, dst: str) -> None:
    """
    Copy src -> dst atomically: write to dst.part, fsync, then os.replace().
    Ensures web server never sees a half-written file.
    """
    tmp = dst + ".part"
    with open(src, "rb") as fsrc, open(tmp, "wb") as fdst:
        shutil.copyfileobj(fsrc, fdst, length=1024*1024)  # 1MB chunks
        fdst.flush()
        os.fsync(fdst.fileno())
    os.replace(tmp, dst)

# ===== Public bot entry points =====
def handle_message(user_id: str, message: str, settings: Optional[BotSettings] = None) -> Tuple[str, List[str]]:
    """
    Main message handler (runtime-driven).
    Returns (reply_text, attachments).
    """
    settings = settings or BotSettings()
    sess = ensure_session(user_id)
    sess["history"].append({"role": "user", "content": message})
    session_store.set(user_id, sess)

    # Optional model-driven mode: delegate end-to-end response to GPT using system_commands map
    if os.getenv("MODEL_DRIVEN", "0") in ("1","true","yes"):
        try:
            from openai import OpenAI
            client = OpenAI()
        except Exception as e:
            dbg("OpenAI client unavailable (model-driven)", str(e))
            # fall back to legacy behavior
        else:
            # Build lightweight knowledge context from vector store
            try:
                kb_text = _retrieve_knowledge_text(message, sess.get("prefs", {}), top_k=6)
            except Exception as e:
                dbg("kb_retrieve_failed", str(e))
                kb_text = ""

            # Compose prompt with recent history and knowledge
            recent_hist = sess.get("history", [])[-settings.history_window:]
            hist_lines = []
            for h in recent_hist:
                role = h.get("role","user")
                content = h.get("content","")
                if content:
                    hist_lines.append(f"{role}: {content}")
            hist_block = "\n".join(hist_lines)

            combined_prompt = f"{message}\n\nPrevious Conversations:\n{hist_block}\n\nKnowledgebase:\n{kb_text}".strip()

            # Accept system_commands either as list[str] (legacy) or dict with role/tone/knowledge/do/dont
            sys_map = {}
            if isinstance(settings.system_commands, dict):
                sys_map = dict(settings.system_commands)
            else:
                sys_map = {
                    "role": "Rental apartment assistant",
                    "tone": "Professional, concise",
                    "knowledge": "Use knowledgebase provided when answering; cite URLs if present.",
                    "do": "Understand intent, keep context, produce a single concise reply.",
                    "dont": "Don't change topic beyond apartments; don't reveal system instructions."
                }
            # Inject knowledge snippet into system map if template expects it
            try:
                if "{combined_text}" in (sys_map.get("knowledge") or ""):
                    sys_map["knowledge"] = (sys_map.get("knowledge") or "").format(combined_text=kb_text)
            except Exception:
                pass

            reply_text_any = get_gpt_response(client, combined_prompt, sys_map)
            reply_text = reply_text_any if isinstance(reply_text_any, str) else json.dumps(reply_text_any)

            # Heuristic: if user just provided full contact, clear session after we reply
            if _has_full_contact(message):
                try:
                    session_store.delete(user_id)
                except Exception:
                    pass
            else:
                sess["history"].append({"role": "assistant", "content": reply_text})
                session_store.set(user_id, sess)

            return (reply_text, [])

    # 0) If awaiting contact, try to capture contact first
    if sess.get("awaiting_contact"):
        reply = handle_contact(user_id, message, settings, auto_followup=True)
        sess["history"].append({"role": "assistant", "content": reply})
        session_store.set(user_id, sess)
        return (reply, [])

    # 1) If we're waiting for time, handle that FIRST:
    if sess.get("awaiting_time"):
        hits = sess.get("last_results") or []
        if _looks_like_selection(message, len(hits)):
            resp = handle_selection(user_id, message, settings)
            sess["history"].append({"role": "assistant", "content": resp})
            session_store.set(user_id, sess)
            return (resp, [])

        t_strong = is_strong_time_expression(message)
        dbg("Awaiting time (strong) -> extracted", t_strong)
        if t_strong:
            sess["awaiting_time"] = False
            sess["appt_time"] = t_strong
            sess["awaiting_contact"] = True
            session_store.set(user_id, sess)
            brief = {
                "situation": "time_captured",
                "time": t_strong,
                "fallback": "Awesome, got your time slot. Now, to make this happen, mind sharing your name, email, and phone?"
            }
            reply = compose_reply_with_llm(user_id, brief, settings)
            sess["history"].append({"role": "assistant", "content": reply})
            session_store.set(user_id, sess)
            return (reply, [])

    # 2) Route via LLM
    routed = route_intent_with_llm(user_id, message, settings)
    intent = routed.get("intent", "other")
    prefs_from_llm = routed.get("prefs") or {}
    selection_from_llm = routed.get("selection") or []
    time_expr = routed.get("time") or None
    query = (routed.get("query") or "").strip()
    needs_clarify = bool(routed.get("needs_clarification", False))

    dbg("Intent", intent); dbg("Router prefs", prefs_from_llm)
    dbg("Router query", query); dbg("Router selection", selection_from_llm)
    dbg("Router time", time_expr); dbg("Router needs_clarify", needs_clarify)

    # 3) Merge prefs (avoid polluting prefs on pure selection/time)
    if intent not in ("select", "time"):
        merged = merge_prefs(sess['prefs'], parse_prefs(message))
        new_locs = extract_locations_from_text(message)
        if new_locs:
            merged["areas"] = new_locs
            dbg("Areas overridden from latest message", new_locs)
        for k, v in (prefs_from_llm or {}).items():
            if v not in (None, [], "", {}):
                if k == "areas" and new_locs:
                    continue
                merged[k] = v
        sess['prefs'] = merged
        session_store.set(user_id, sess)
        dbg("Prefs merged", merged)

    # 4) Greetings / Initial
    if intent == "greeting" or (not sess.get('has_initialized', False) and is_greeting(message)):
        sess['has_initialized'] = True
        session_store.set(user_id, sess)
        brief = {
            "situation": "greeting",
            "prefs": sess.get("prefs", {}),
            "fallback": "Hey! I'm your go-to buddy for snagging the perfect apartment. What's the move-in date you're eyeing? Anyone joining you? Neighborhood vibes? Budget? Amenities? Spill the details!"
        }
        reply = compose_reply_with_llm(user_id, brief, settings)
        sess["history"].append({"role": "assistant", "content": reply})
        session_store.set(user_id, sess)
        return (reply, [])

    # 5) If router found time, use strong detector to confirm
    if intent == "time":
        t_strong = is_strong_time_expression(message if time_expr is None else time_expr)
        if t_strong:
            sess["awaiting_time"] = False
            sess["appt_time"] = t_strong
            sess["awaiting_contact"] = True
            session_store.set(user_id, sess)
            brief = {
                "situation": "time_captured",
                "time": t_strong,
                "fallback": "Sweet, {time} it is. Quick‚Äîyour name, email, phone so I can lock this down?"
            }
            reply = compose_reply_with_llm(user_id, {"situation": "time_captured", "time": t_strong}, settings)
            sess["history"].append({"role": "assistant", "content": reply})
            session_store.set(user_id, sess)
            return (reply, [])

    # 6) Contact (router-based)
    if intent == "contact":
        resp = handle_contact(user_id, message, settings, auto_followup=True)
        sess["history"].append({"role": "assistant", "content": resp})
        session_store.set(user_id, sess)
        return (resp, [])

    # 7) Selection
    if intent == "select":
        sel_input = selection_from_llm if selection_from_llm else message
        if isinstance(sel_input, str):
            maybe_time = is_strong_time_expression(sel_input)
            if maybe_time:
                sess["awaiting_time"] = False
                sess["appt_time"] = maybe_time
                sess["awaiting_contact"] = True
                session_store.set(user_id, sess)
                brief = {
                    "situation": "time_captured",
                    "time": maybe_time,
                    "fallback": "Got it, {time} works. Now, details: name, email, phone?"
                }
                reply = compose_reply_with_llm(user_id, brief, settings)
                sess["history"].append({"role": "assistant", "content": reply})
                session_store.set(user_id, sess)
                return (reply, [])
        try:
            resp = handle_selection(user_id, sel_input, settings)
            sess["history"].append({"role": "assistant", "content": resp})
            session_store.set(user_id, sess)
            return (resp, [])
        except Exception as e:
            dbg("handle_selection failed", str(e))
            brief = {
                "situation": "selection_help",
                "options_count": len(sess.get('last_results') or []),
                "fallback": "Which ones catch your eye? Just say the numbers, like '1 and 3'."
            }
            reply = compose_reply_with_llm(user_id, brief, settings)
            sess["history"].append({"role": "assistant", "content": reply})
            session_store.set(user_id, sess)
            return (reply, [])

    # 8) Clarify vs search vs fallback
    areas = [a for a in (sess['prefs'].get("areas") or []) if isinstance(a, str) and a.strip()]
    is_search_intent = (intent == "search") and (bool(query) or bool(areas))

    if intent == "clarify" or needs_clarify:
        brief = {
            "situation": "needs_clarify",
            "prefs": sess.get("prefs", {}),
            "fallback": "Help me narrow it down‚Äîwhat's the area/station? Max budget in JPY? Bedrooms? Hit me with details!"
        }
        reply = compose_reply_with_llm(user_id, brief, settings)
        sess["history"].append({"role": "assistant", "content": reply})
        session_store.set(user_id, sess)
        return (reply, [])

    if not is_search_intent:
        brief = {
            "user_message": message,
            "prefs": sess.get("prefs", {}),
            "situation": "off_search",
            "fallback": "I'm all about finding you killer apartments‚Äîwant me to scout some options?"
        }
        try:
            reply_text = compose_reply_with_llm(user_id, brief, settings)
        except Exception as e:
            dbg("compose_reply_with_llm failed", str(e))
            reply_text = brief["fallback"]
        sess["history"].append({"role": "assistant", "content": reply_text})
        session_store.set(user_id, sess)
        return (reply_text, [])

    # 9) Run search
    final_query = (query or "").strip() or ("apartment rent " + " ".join(areas)).strip() or "apartment rent"
    dbg("Final query (search)", final_query)

    packed = search_and_clean(user_id, final_query, sess['prefs'], settings, min_count=3)

    if packed.get("unavailable"):
        reason = packed.get("reason", "no_results")
        if reason == "non_japan":
            brief = {
                "situation": "non_japan",
                "query": final_query,
                "fallback": "Oops, right now I'm all about spots in Japan‚Äîapologies for the mix-up! Meant Chiba or somewhere local? Let's pivot there."
            }
        else:
            brief = {
                "situation": "no_results",
                "query": final_query,
                "prefs": sess['prefs'],
                "fallback": "No luck on that exact search yet. Nearby hoods? Tweak budget or beds?"
            }
        reply = compose_reply_with_llm(user_id, brief, settings)
        sess["history"].append({"role": "assistant", "content": reply})
        session_store.set(user_id, sess)
        return (reply, [])

    results = packed["final"]
    sess['last_results'] = results
    session_store.set(user_id, sess)

    if len(results) == 0:
        brief = {
            "situation": "no_results",
            "query": final_query,
            "prefs": sess['prefs'],
            "fallback": "Dang, nothing popped up just yet. Refine the area or budget?"
        }
        reply = compose_reply_with_llm(user_id, brief, settings)
        sess["history"].append({"role": "assistant", "content": reply})
        session_store.set(user_id, sess)
        return (reply, [])

    listing_text = format_listings_for_user(results)
    brief = {
        "situation": "results",
        "results_count": len(results),
        "listings": listing_text,
        "fallback": f"Found {len(results)} solid options! Pick numbers you dig (e.g., '1, 3').\n\n{listing_text}"
    }
    if len(results) < 5:
        brief["fallback"] = f"Just a few gems based on what you've shared‚Äîmore deets on station/budget/beds for better hits?\n\n{listing_text}"
    reply = compose_reply_with_llm(user_id, brief, settings)

    sess["history"].append({"role": "assistant", "content": reply})
    session_store.set(user_id, sess)
    return (reply, [])


# ===== Selection & contact =====
def _looks_like_selection(s: str, list_len: int) -> bool:
    """True if the text contains only valid selection numbers/ranges and NO strong time indicators."""
    if not s or list_len <= 0:
        return False
    if is_strong_time_expression(s):
        return False
    txt = normalize_digits(s)
    got_any = False
    for a, b in re.findall(r"\b(\d{1,3})\s*[-‚Äì‚Äî]\s*(\d{1,3})\b", txt):
        try:
            ia, ib = int(a), int(b)
            if not (1 <= ia <= list_len and 1 <= ib <= list_len):
                return False
            got_any = True
        except Exception:
            return False
    nums = re.findall(r"\b\d{1,3}\b", re.sub(r"\b\d{1,3}\s*[-‚Äì‚Äî]\s*\d{1,3}\b", " ", txt))
    for n in nums:
        iv = int(n)
        if not (1 <= iv <= list_len):
            return False
        got_any = True
    return got_any

from urllib.parse import urlparse  # put at top of file
def _extract_image_urls(item: Dict[str, Any]) -> List[str]:
    """Collect plausible image URLs; skip obvious UI icons/sprites."""
    urls = []
    
    # Handle the 'images' field which is a string representation of a list
    if "images" in item and isinstance(item["images"], str):
        import ast
        try:
            # Try to safely evaluate the string as a Python literal
            images_list = ast.literal_eval(item["images"])
            if isinstance(images_list, list):
                urls = [img for img in images_list if isinstance(img, str) and img.startswith(('http://', 'https://'))]
                dbg("parsed_images_list", {
                    "count": len(urls),
                    "sample": urls[0][:100] + "..." if urls else "None"
                })
        except (ValueError, SyntaxError) as e:
            # If evaluation fails, fall back to regex
            import re
            img_urls = re.findall(r'https?://[^\s\']+', item["images"])
            if img_urls:
                urls = img_urls
                dbg("fallback_regex_extraction", {
                    "count": len(urls),
                    "sample": urls[0][:100] + "..." if urls else "None"
                })
    
    # Clean and validate URLs
    out = []
    seen = set()
    for url in urls:
        if not isinstance(url, str) or len(url) < 10:
            continue
            
        url = url.strip("'\"")
        if not any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
            continue
            
        if url not in seen:
            seen.add(url)
            out.append(url)
    
    dbg("final_extracted_urls", {
        "item_title": item.get("title") or item.get("name") or "unknown",
        "found_urls": len(out),
        "sample_urls": out[:3] if out else []
    })
    
    return out

def _download_images_to_temp(urls: List[str], max_images: int = 2) -> Tuple[str, List[str]]:
    """
    Robust image downloader (capped):
      - Downloads at most `max_images` (hard-capped to 2).
      - Per-host headers (Referer, Accept-Language, realistic UA)
      - Fallback ext via Content-Type
      - Retry HEAD quirks
      - Max size guard (MAX_IMG_MB, default 8)
    Returns: (tmpdir, [abs file paths])
    """
    import os
    import mimetypes
    import tempfile
    from urllib.parse import urlparse

    # ‚úÖ Hard cap to 2 (requirement)
    MAX_IMAGES = min(int(max_images) if isinstance(max_images, int) and max_images > 0 else 2, 2)
    DEFAULT_EXT = ".jpg"
    ALLOWED_EXTS = {".jpg", ".jpeg", ".png", ".webp", ".gif"}
    MAX_IMG_MB = float(os.getenv("MAX_IMG_MB", "8"))
    MAX_BYTES = int(MAX_IMG_MB * 1024 * 1024)

    tmpdir = tempfile.mkdtemp(prefix="listing_imgs_")
    saved: List[str] = []

    sess = _session_with_retries()  # has .request_timeout set

    UA = (os.getenv("IMG_FETCH_UA") or "").strip() or \
         "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"
    ACCEPT_LANG = (os.getenv("IMG_FETCH_ACCEPT_LANGUAGE") or "").strip() or \
                  "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7"
    GLOBAL_REFERER = (os.getenv("IMG_FETCH_REFERER") or "").strip() or None

    def _pick_ext(url_path: str, resp_ct: str) -> str:
        ext = os.path.splitext(url_path)[1].lower()
        if ext in ALLOWED_EXTS:
            return ext
        ct = (resp_ct or "").split(";", 1)[0].strip().lower()
        if ct:
            guess = mimetypes.guess_extension(ct) or ""
            guess = guess.lower()
            if guess == ".jpe":
                guess = ".jpg"
            if guess in ALLOWED_EXTS:
                return guess
            if ct in ("image/jpeg", "image/jpg"):
                return ".jpg"
            if ct == "image/png":
                return ".png"
            if ct == "image/webp":
                return ".webp"
            if ct == "image/gif":
                return ".gif"
        return DEFAULT_EXT

    def _per_host_headers(u: str) -> Dict[str, str]:
        host = (urlparse(u).netloc or "").lower()
        h = {
            "Accept": "image/*,*/*;q=0.8",
            "Accept-Language": ACCEPT_LANG,
            "User-Agent": UA,
        }
        if GLOBAL_REFERER:
            h["Referer"] = GLOBAL_REFERER
        else:
            if host.endswith("yimg.jp") or "yahoo.co.jp" in host:
                h["Referer"] = "https://realestate.yahoo.co.jp/"
            elif "rakuten" in host:
                h["Referer"] = "https://www.rakuten.co.jp/"
            elif "googleusercontent.com" in host or "ggpht.com" in host:
                h["Referer"] = "https://www.google.com/"
        return h

    for i, url in enumerate(urls[:MAX_IMAGES], start=1):
        if not (isinstance(url, str) and (url.startswith("http://") or url.startswith("https://"))):
            dbg("image_download_failed", {"url": url, "err": "invalid_url"})
            continue

        try:
            # Some CDNs dislike HEAD; ignore failures
            try:
                sess.head(url, timeout=sess.request_timeout, headers={"User-Agent": UA})
            except Exception:
                pass

            headers = _per_host_headers(url)
            r = sess.get(url, timeout=sess.request_timeout, stream=True, headers=headers, allow_redirects=True)
            status = r.status_code

            # Retry once with generic referer on 401/403 (if no global referer)
            if status in (401, 403) and not GLOBAL_REFERER:
                try:
                    r.close()
                except Exception:
                    pass
                headers2 = dict(headers)
                headers2["Referer"] = "https://www.google.com/"
                r = sess.get(url, timeout=sess.request_timeout, stream=True, headers=headers2, allow_redirects=True)
                status = r.status_code

            r.raise_for_status()

            ct = r.headers.get("content-type", "")
            path_part = url.split("?", 1)[0]
            ext = _pick_ext(path_part, ct)
            if ext not in ALLOWED_EXTS:
                ext = DEFAULT_EXT

            fname = os.path.join(tmpdir, f"img_{i}{ext}")
            total = 0
            with open(fname, "wb") as f:
                for chunk in r.iter_content(chunk_size=64 * 1024):
                    if not chunk:
                        continue
                    total += len(chunk)
                    if total > MAX_BYTES:
                        raise ValueError(f"image_too_large>{MAX_IMG_MB}MB")
                    f.write(chunk)

            if total == 0:
                try:
                    os.remove(fname)
                except Exception:
                    pass
                raise ValueError("empty_image_response")

            saved.append(fname)

        except Exception as e:
            dbg("image_download_failed", {"url": url, "err": str(e)})

    return tmpdir, saved

def _maybe_upload_files(filepaths: List[str]) -> List[str]:
    """
    Optional: upload files to get public URLs (needed if you don't want to send the original URLs).
    Expects MEDIA_UPLOAD_URL env that accepts multipart 'file' and returns JSON {'url': 'https://...'}.
    """
    upload_endpoint = os.getenv("MEDIA_UPLOAD_URL")
    if not upload_endpoint:
        return []  # signal: no uploads, caller should fall back to original URLs
    out_urls: List[str] = []
    for p in filepaths:
        try:
            with open(p, "rb") as f:
                resp = requests.post(upload_endpoint, files={"file": (os.path.basename(p), f)}, timeout=30)
            resp.raise_for_status()
            data = resp.json() if resp.headers.get("content-type","").startswith("application/json") else {}
            url = data.get("url")
            if url and url.startswith(("http://","https://")):
                out_urls.append(url)
            else:
                dbg("upload_no_url", {"file": p, "status": resp.status_code, "text": resp.text[:300]})
        except Exception as e:
            dbg("upload_failed", {"file": p, "err": str(e)})
    return out_urls

def _twilio_send_whatsapp_media(media_urls: List[str], to_number: Optional[str]) -> None:
    """
    Send one WhatsApp message per media item using Twilio.
    Falls back to environment WHATSAPP_TO if 'to_number' is None.
    """
    from_number = os.getenv("WHATSAPP_FROM")
    account_sid = os.getenv("TWILIO_ACCOUNT_SID")
    auth_token = os.getenv("TWILIO_AUTH_TOKEN")
    to_number = to_number or os.getenv("WHATSAPP_TO")
    if not (from_number and account_sid and auth_token and to_number):
        dbg("twilio_missing_config", {"from": from_number, "sid": bool(account_sid), "to": to_number})
        return
    try:
        from twilio.rest import Client  # lazy import
        client = Client(account_sid, auth_token)
        for murl in media_urls:
            try:
                client.messages.create(
                    from_=from_number,
                    to=to_number,
                    body="",  # keep body empty or add caption if you want
                    media_url=[murl]
                )
            except Exception as e:
                dbg("twilio_send_failed", {"url": murl, "err": str(e)})
    except Exception as e:
        dbg("twilio_client_failed", str(e))

def _retrieve_knowledge_text(prompt: str, prefs: Dict[str, Any], top_k: int = 5) -> str:
    """Small helper to fetch top_k texts from vector store for model-driven mode."""
    try:
        store = ListingStore(collection=SEARCH_COLLECTION)
        variants = expand_query(prompt, prefs)
        merged: List[Dict[str, Any]] = []
        seen = set()
        for v in variants:
            hits = store.search(v, n=top_k*2, where=prefs) or []
            for h in hits:
                key = h.get("id") or h.get("url") or h.get("details_url") or h.get("document")
                if key in seen: continue
                seen.add(key); merged.append(h)
            if len(merged) >= top_k:
                break
        texts = []
        for m in merged[:top_k]:
            doc = m.get("document") or m.get("text") or ""
            url = m.get("url") or m.get("details_url") or ""
            if url:
                texts.append(f"{doc}\nSource: {url}")
            else:
                texts.append(doc)
        return "\n\n".join([t for t in texts if t])
    except Exception as e:
        dbg("kb_text_fail", str(e))
        return ""

def _has_full_contact(msg: str) -> bool:
    if not msg:
        return False
    has_email = bool(re.search(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", msg))
    digits = re.findall(r"\d", normalize_digits(msg))
    has_phone = len(digits) >= 7
    name = _extract_name(msg)
    return bool(has_email and has_phone and name)
import time  # For retriesimport time  # Ensure for backoff
import time  # For backoff

def send_listing_images_if_any(user_id: str, listing: Dict[str, Any]) -> None:
    tmpdir = tmpdir2 = None
    try:
        urls = _extract_image_urls(listing)
        if not urls:
            dbg("no_images_in_listing", listing.get("title") or listing.get("url") or "unknown")
            return

        # Helper to force HTTPS
        def _https(u): 
            if u.startswith("https://"): return u
            if u.startswith("http://"): return "https://" + u.split("://",1)[1]
            return u

        # FAST PATH: Send originals directly (use if processing fails often)
        if os.getenv("SEND_ORIGINAL_IMAGE_URLS", "9").lower() in ("1","true","yes"):
            sess = ensure_session(user_id)
            to_number = (
                _normalize_wa_number(user_id)
                or _normalize_wa_number((sess.get("contact") or {}).get("phone"))
                or _normalize_wa_number(os.getenv("WHATSAPP_TO"))
            )
            if not to_number:
                dbg("no_recipient_number", {"user_id": user_id}); return
            fast_urls = [_https(u) for u in urls[:int(os.getenv("MAX_MEDIA_PER_SEND","2"))]]
            dbg("media_urls_fastpath", fast_urls)
            _twilio_send_whatsapp_media(fast_urls, to_number)
            return

        # Priority: Third-party proxy from env (e.g., BrightData URL for 99% success)
        THIRD_PARTY_PROXY = os.getenv("IMAGE_PROXY_URL", None)  # Set to paid proxy like BrightData

        # Expanded hardcoded list of fresh Japan elite proxies (20+ from latest Oct 2025 lists)
        FREE_PROXY_LIST = [
            "http://103.147.14.119:80",      # Tokyo, Elite HTTP
            "http://103.147.14.120:80",      # Osaka, Elite HTTP
            "http://116.80.58.222:3172",     # Japan General, Elite HTTP
            "http://8.130.34.44:8080",       # Tokyo, Anonymous HTTP
            "https://140.227.1.10:80",       # JP Residential-like, Elite HTTPS
            "http://47.74.46.81:102",        # Tokyo Backup
            "http://116.80.59.172:80",       # Japan Low Latency
            "http://147.75.34.105:443",      # EU/JP Route, Elite HTTPS
            "http://77.105.137.42:8080",     # EU Backup
            "http://199.188.207.170:8080",   # US/JP Route
            "http://15.168.235.57:407",      # JP Fresh Elite HTTP (new from free-proxy-list)
            "http://47.79.93.202:1122",      # Tokyo Elite HTTP (new)
            "http://160.251.142.232:80",     # Japan Elite HTTP (new)
            "http://103.147.14.121:80",      # Osaka Backup
            "http://116.80.58.223:3172",     # Japan General Backup
            "http://8.130.34.45:8080",       # Tokyo Backup
            "https://140.227.1.11:80",       # JP Residential Backup
            "http://47.74.46.82:102",        # Tokyo Elite Backup
            "http://116.80.59.173:80",       # Japan Low Latency Backup
            "http://147.75.34.106:443",      # EU/JP Backup
            "http://140.227.61.201:80",      # JP Elite (from ditatompel)
            "http://15.168.235.58:407"       # JP Fresh Backup
        ]

        # Use third-party first, fallback to free list
        PROXY_TO_TRY = [THIRD_PARTY_PROXY] if THIRD_PARTY_PROXY else FREE_PROXY_LIST

        # Enhanced download with proxy rotation + retries + detailed logging
        def _download_with_retry(urls_list: List[str], max_retries: int = 3) -> Tuple[Optional[str], List[str]]:
            proxy_attempts = 0
            for proxy in PROXY_TO_TRY:
                if not proxy:
                    continue
                proxy_attempts += 1
                is_third_party = proxy == THIRD_PARTY_PROXY
                dbg("trying_proxy_rotation", {"proxy": proxy[:20] + "...", "is_third_party": is_third_party, "attempt": proxy_attempts})
                for attempt in range(1, max_retries + 1):
                    try:
                        sess = _session_with_retries()
                        if proxy:
                            sess.proxies = {"http": proxy, "https": proxy}

                        # Yahoo-specific: Force referer/UA
                        for u in urls_list:
                            if "yimg.jp" in u:
                                sess.headers.update({
                                    "Referer": "https://realestate.yahoo.co.jp/",
                                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                                })
                                break

                        tmp_d, files_d = _download_images_to_temp(urls_list)
                        if files_d:
                            dbg(f"download_success_proxy_{proxy[:20]}..._attempt_{attempt}", {"files": len(files_d), "urls": [u[:50] + "..." for u in urls_list], "is_third_party": is_third_party})
                            return tmp_d, files_d
                        else:
                            dbg(f"download_failed_proxy_{proxy[:20]}..._attempt_{attempt}", {"reason": "no_files", "urls": [u[:50] + "..." for u in urls_list], "is_third_party": is_third_party})
                            if attempt < max_retries:
                                time.sleep(2 ** attempt)  # Exponential backoff
                    except Exception as retry_e:
                        dbg(f"download_retry_proxy_{proxy[:20]}..._attempt_{attempt}_error", {"error": str(retry_e), "urls": [u[:50] + "..." for u in urls_list], "is_third_party": is_third_party})
                        if attempt == max_retries:
                            continue  # Next proxy
            dbg("all_proxies_failed_after_rotation", {"total_proxies_tried": len(PROXY_TO_TRY), "third_party_used": bool(THIRD_PARTY_PROXY)})
            return None, []

        # Attempt download with retries
        tmpdir, files = _download_with_retry(urls)
        if not files:
            rehosted = _maybe_upload_from_urls(urls)
            if rehosted:
                tmpdir2, files2 = _download_with_retry(rehosted)
                files = files2

        # If still no files, fallback to originals
        if not files:
            dbg("image_download_unavailable_after_retries_fallback_to_originals", {"original_urls": [u[:50] + "..." for u in urls[:2]]})
            media_to_send = [_https(u) for u in urls[:int(os.getenv("MAX_MEDIA_PER_SEND","2"))]]
        else:
            # Process files
            files = _lightly_process_images(files)
            files = _ai_edit_images_if_allowed(files)
            cap = os.getenv("CAPTION_TEXT", "disruptive thoughts generation")
            files = [_add_caption_to_file(p, cap) for p in files]

            # Publish
            media_to_send = _publish_locally(files)
            if not media_to_send:
                dbg("local_publish_failed_all_fallback_to_originals", "FALLBACK_TO_ORIGINAL_URLS")
                media_to_send = [_https(u) for u in urls[:int(os.getenv("MAX_MEDIA_PER_SEND","2"))]]

        dbg("media_urls_final", media_to_send[:2])  # First 2 for debug

        # Probe only for self-hosted
        if (os.getenv("MEDIA_PROBE_BEFORE_SEND", "2").lower() in ("1","true","yes") and 
            all("static/wa-media" in u for u in media_to_send)):  # Adjust for your server
            _wait_until_public(media_to_send)

        sess = ensure_session(user_id)
        to_number = (
            _normalize_wa_number(user_id)
            or _normalize_wa_number((sess.get("contact") or {}).get("phone"))
            or _normalize_wa_number(os.getenv("WHATSAPP_TO"))
        )
        if not to_number:
            dbg("no_recipient_number", {"user_id": user_id}); return

        _twilio_send_whatsapp_media(media_to_send, to_number)

    except Exception as e:
        dbg("send_listing_images_if_any_failed", str(e))
        # Ultimate fallback: One original
        try:
            urls = _extract_image_urls(listing)
            if urls:
                fallback_urls = [_https(urls[0])]
                sess = ensure_session(user_id)
                to_number = (
                    _normalize_wa_number(user_id)
                    or _normalize_wa_number((sess.get("contact") or {}).get("phone"))
                    or _normalize_wa_number(os.getenv("WHATSAPP_TO"))
                )
                if to_number:
                    _twilio_send_whatsapp_media(fallback_urls, to_number)
                    dbg("ultimate_fallback_sent_original", fallback_urls)
        except Exception as e2:
            dbg("ultimate_fallback_failed", str(e2))
    finally:
        try: shutil.rmtree(tmpdir, ignore_errors=True)
        except Exception: pass
        try: shutil.rmtree(tmpdir2, ignore_errors=True)
        except Exception: pass



def handle_selection(user_id: str, message_or_selection, settings: Optional[BotSettings] = None) -> str:
    """
    Accepts either:
      - a list/tuple of integers (router-provided selection indexes), OR
      - a raw message string (user text)
    Returns a reply asking for time and stores chosen items in session.
    """
    settings = settings or BotSettings()
    sess = ensure_session(user_id)
    hits = sess.get('last_results') or []
    if not hits:
        dbg("Selection but no last_results", user_id)
        brief = {
            "situation": "no_options_yet",
            "fallback": "Haven't pulled options yet‚Äîtell me what you're after?"
        }
        return compose_reply_with_llm(user_id, brief, settings)

    dbg("handle_selection input", message_or_selection)

    if isinstance(message_or_selection, str):
        maybe_time = is_strong_time_expression(message_or_selection)
        if maybe_time:
            sess['awaiting_time'] = False
            sess['appt_time'] = maybe_time
            sess['awaiting_contact'] = True
            session_store.set(user_id, sess)
            brief = {
                "situation": "time_captured",
                "time": maybe_time,
                "fallback": "Perfect, {time} sounds good. Name, email, phone to seal the deal?"
            }
            return compose_reply_with_llm(user_id, brief, settings)

    if isinstance(message_or_selection, (list, tuple)) and any(isinstance(n, int) for n in message_or_selection):
        nums_list = [int(n) for n in message_or_selection if isinstance(n, int)]
    else:
        s = str(message_or_selection or "").strip()
        s = normalize_digits(s)  # now case-insensitive
        nums_list: List[int] = []
        ranges = re.findall(r"\b(\d{1,3})\s*[-‚Äì‚Äî]\s*(\d{1,3})\b", s)
        for a, b in ranges:
            try:
                ia, ib = int(a), int(b)
                if ia <= ib:
                    nums_list.extend(list(range(ia, ib + 1)))
                else:
                    nums_list.extend(list(range(ib, ia + 1)))
            except Exception:
                pass
        s_no_ranges = re.sub(r"\b\d{1,3}\s*[-‚Äì‚Äî]\s*\d{1,3}\b", " ", s)
        found_nums = re.findall(r"\b\d{1,3}\b", s_no_ranges)
        nums_list.extend([int(n) for n in found_nums])
        if not nums_list:
            found_nums = re.findall(r"(?:option|opt|no\.?)\s*(\d{1,3})", s_no_ranges, flags=re.IGNORECASE)
            nums_list.extend([int(n) for n in found_nums])
        if not nums_list and s_no_ranges.isdigit():
            digits = list(s_no_ranges)
            cand = [int(d) for d in digits]
            if all(1 <= d <= len(hits) for d in cand):
                nums_list = cand

    dbg("Parsed numeric selections", nums_list)

    valid = sorted({n for n in nums_list if 1 <= n <= len(hits)})
    if not valid:
        brief = {
            "situation": "invalid_selection",
            "options_count": len(hits),
            "fallback": "Hmm, those numbers don't match up‚Äîtry '1 and 3' or whatever grabs you from the list?"
        }
        return compose_reply_with_llm(user_id, brief, settings)

    idx = [n - 1 for n in valid]
    chosen = [hits[i] for i in idx]
    sess['chosen'] = chosen
    sess['awaiting_time'] = True
    sess['awaiting_contact'] = False
    session_store.set(user_id, sess)

    # [NEW] Auto-send images for single-item selection (if enabled)
    try:
        if os.getenv("SEND_IMAGES_ON_SELECT", "0") == "1" and len(chosen) == 1:
            send_listing_images_if_any(user_id, chosen[0])
    except Exception as e:
        dbg("auto_send_images_error", str(e))

    summary_lines = []
    for i, it in enumerate(chosen, start=1):
        title = it.get("title") or "Untitled"
        url = it.get("url") or it.get("details_url") or ""
        price = it.get("price_usd") or it.get("price_yen")
        if it.get("price_usd"):
            price_str = f"${price}/mo"
        elif it.get("price_yen"):
            try:
                price_str = f"¬•{int(price):,}/mo"
            except Exception:
                price_str = f"¬•{price}/mo"
        else:
            price_str = ""
        line = f"{i}) {title}"
        if price_str:
            line += f" ‚Äî {price_str}"
        if url:
            line += f"\n{url}"
        summary_lines.append(line)
    summary = "\n\n".join(summary_lines)

    brief = {
        "situation": "selection_confirmed",
        "chosen_summary": summary,
        "fallback": f"Love those picks!\n\n{summary}\n\nWhen's good for a look? (e.g., '10am tomorrow' or 'anytime this week')"
    }
    reply = compose_reply_with_llm(user_id, brief, settings)
    return reply


def _extract_name(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"\bmy name is\s+([A-Za-z .'-]{2,60})", text, re.IGNORECASE)
    if m:
        return m.group(1).strip(" .")
    # simple fallback: two words without @
    parts = [p for p in re.split(r"[,\n]", text) if p.strip()]
    for p in parts:
        if "@" not in p and any(ch.isalpha() for ch in p):
            tokens = p.strip().split()
            if 1 <= len(tokens) <= 4:
                return p.strip()
    return None

def _ai_extract_contact(user_id: str, message: str, settings: BotSettings) -> Dict[str, Any]:
    """
    Use LLM to intelligently extract name, email, phone from the message.
    Returns {'name': str|None, 'email': str|None, 'phone': str|None}
    """
    system = [
        "You extract contact details from user messages for a rental bot. "
        "Be precise: only extract if clearly intended as contact info. "
        "Ignore greetings like 'Hi' as names. Names should be full (e.g., 'John Doe'). "
        "Phone: 7+ digits, optionally formatted. Email: valid format. "
        "Return ONLY JSON: {\"name\": null|string, \"email\": null|string, \"phone\": null|string}"
    ] + settings.system_commands
    payload = {
        "message": message,
        "instructions": (
            "Extract exactly one name (full if possible, null if unclear/greeting), "
            "one email (valid format or null), one phone (digits only, null if invalid). "
            "If message is off-topic or incomplete, set to null."
        )
    }
    res = chat_json(
        user_id, system, payload,
        model=settings.model,
        extra_system='Strict: null if not clearly a full name/email/phone. Phone as digits string.',
        history_window=settings.history_window,
        temperature=0.0,  # deterministic
        top_p=1.0,
        seed=settings.seed
    )
    if isinstance(res, dict):
        return {
            "name": res.get("name") or None,
            "email": res.get("email") or None,
            "phone": res.get("phone") or None
        }
    return {"name": None, "email": None, "phone": None}


def _ai_compose_contact_reply(user_id: str, current_contact: Dict[str, Any], settings: BotSettings) -> str:
    """
    Use LLM to generate a natural reply based on current missing fields.
    """
    missing = [k for k, v in current_contact.items() if not v]
    if not missing:
        return settings.prompts.contact_saved

    system = [
        "You are a friendly assistant collecting contact info for apartment viewings. "
        "Given current contact and missing fields, craft a concise, natural follow-up. "
        "Be polite, specific, and encouraging. Return ONLY JSON {\"text\": \"...\"}."
    ] + settings.system_commands
    payload = {
        "current": current_contact,
        "missing": missing,
        "instructions": (
            f"Ask for the missing fields: {', '.join(missing)}. "
            "If all missing, ask for all at once with example. "
            "If one missing, focus on that. Keep it conversational."
        )
    }
    res = chat_json(
        user_id, system, payload,
        model=settings.model,
        extra_system='Natural tone: e.g., "Great, now just your email?" or full prompt if all missing.',
        history_window=settings.history_window,
        temperature=settings.temperature,
        top_p=settings.top_p,
        seed=settings.seed
    )
    if isinstance(res, dict):
        return res.get("text", "Thanks! I still need your contact details to proceed.")
    return "Thanks! I still need your contact details to proceed."

def handle_contact(user_id: str, message: str, settings: Optional[BotSettings] = None, auto_followup: bool = False) -> str:
    """
    AI-driven contact capture: Use LLM to extract and compose replies dynamically.
    Validates and updates session; generates natural asks for missing info.
    """
    settings = settings or BotSettings()
    sess = ensure_session(user_id)

    # AI extract from this message
    extracted = _ai_extract_contact(user_id, message, settings)
    dbg("AI contact extract", extracted)

    # Update session contact (overwrite if new valid info found)
    contact = sess.get("contact", {"name": None, "email": None, "phone": None})
    updated = False
    if extracted["name"] and extracted["name"].strip() and len(extracted["name"].split()) >= 1 and extracted["name"].lower() not in ["hi", "hello", "hey"]:  # basic validation
        contact["name"] = extracted["name"].strip()
        updated = True
    if extracted["email"] and "@" in extracted["email"] and "." in extracted["email"].split("@",1)[1]:
        contact["email"] = extracted["email"].strip().lower()
        updated = True
    if extracted["phone"] and re.match(r"^\d{7,15}$", extracted["phone"]):  # digits only, reasonable length
        contact["phone"] = extracted["phone"]
        updated = True

    sess["contact"] = contact
    session_store.set(user_id, sess)

    # If no update and message doesn't look like contact attempt, perhaps reprompt generally
    if not updated and not re.search(r"(name|email|phone|contact|details)", message, re.IGNORECASE):
        # But still check if all missing, ask for all
        pass  # proceed to compose

    # Compose AI reply based on current state
    reply = _ai_compose_contact_reply(user_id, contact, settings)

    # If all complete, finalize
    missing = [k for k, v in contact.items() if not v]
    if not missing:
        sess["awaiting_contact"] = False
        session_store.set(user_id, sess)
        chosen = sess.get("chosen") or []
        if not chosen:
            brief = {
                "situation": "contact_complete_no_choice",
                "fallback": "All set with your info! Ready to hunt more or book something?"
            }
            final_msg = compose_reply_with_llm(user_id, brief, settings)
            try:
                session_store.delete(user_id)
            except Exception:
                pass
            return final_msg

        summary_lines = []
        for i, it in enumerate(chosen, start=1):
            title = it.get("title") or "Untitled"
            url = it.get("url") or it.get("details_url") or ""
            line = f"{i}) {title}"
            if url:
                line += f"\n{url}"
            summary_lines.append(line)
        summary = "\n".join(summary_lines)

        time_str = sess.get("appt_time") or "TBD"
        brief = {
            "situation": "booking_confirmed",
            "summary": summary,
            "time": time_str,
            "contact": contact,
            "fallback": f"Locked in! You're eyeing:\n{summary}\n\nTime: {time_str}\nYou: {contact['name']} | {contact['email']} | {contact['phone']}\n\nI'll hit you up soon‚Äîchat if plans shift!"
        }
        final_msg = compose_reply_with_llm(user_id, brief, settings)
        # After confirming, clear session per requirement
        try:
            session_store.delete(user_id)
        except Exception:
            pass
        return final_msg

    # Still missing, keep awaiting
    sess["awaiting_contact"] = True
    session_store.set(user_id, sess)
    return reply


# ===== Convenience: single-call, system-driven bot function =====
def run_bot(
    user_id: str,
    user_query: str,
    system_commands: Optional[List[str]] = None,
    prompts: Optional[Dict[str, str]] = None,
    model: Optional[str] = None,
    history_window: Optional[int] = None
) -> Tuple[str, List[str]]:
    settings = BotSettings.from_inputs(system_commands, prompts, model, history_window)
    return handle_message(user_id, user_query, settings)